\newthought{Clearly the nervous system is not solely interested in estimating the state of the world.} Furthermore, if that estimate is not useful for making decisions and taking actions in a dynamic environment, there is little use for it. In the previous chapter I have discussed findings for spiking codes in an estimation context. In this chapter I will extend this approach to the framework of stochastic optimal control, and discuss how to reframe the findings in this context.\par

\section{Estimation and the Separation Principle}

In the previous two chapters, I have considered the filtering problem based on spike trains. More specifically, given a signal, we were looking for the optimal set of parameters for a population of neurons $\varphi^*$ that minimize the MSE of the filtering problem. If we are interested in controlling a system, say a limb performing a movement, the picture changes somewhat. It is very usual for control problem to use the separation principle\cite{Bar-Shalom1974} to develop approximate solutions to control problem. The separation principle considers a control problem with incomplete information as a sequence of two independent problems. The first one is to estimate the state from the observed processes. The second one is to solve some complete information control problem associated to the incomplete information problem we are considering. Then the separation approach to control would be to apply the complete information optimal control on the mean estimate of our state. This approach, and a stronger version of it, the Certainty Equivalence Principle, hold in a number of situations, making the separation principle one of the most important tools in control theory. However, even if one can separate both steps in the control setting, one can not expect optimal encoders for the filtering problem to yield optimal encoders for the control problem.\par
I will in this chapter consider control problems where the system is observed through noisy spike trains. The optimal encoding strategies will in this case be the ones that minimize the control cost, not the MSE. Though these quantities turn out to be related when the state cost is quadratic, I will show a couple of simple examples where the MSE-optimal encoder is drastically different from the control-optimal encoder. Although these results are quite intuitive, as one expects sensory systems to devote more energy to coding for aspects of the environment which are important for their well-being or survival, this approach has been completely overlooked in the optimal population coding literature.

\section{Optimal Control}
The field of control theory is concerned with the steering and controlling of systems, always with the minimization of a cost (or maximization of a reward) in mind. Speaking mathematically, given a system $X$, with dynamics given by
$$
\dot{X}(t) = f(X(t),U(t)),
$$
we would like to select the control variables $U(t)$ in such a way as to minimize an integrated cost function over time
$$
C(X,u,t) = \int_{t}^T c(X(s),u(s),s) ds.
$$
The solution of a control problem is frequently given as a policy $\pi$, a function of the state space to the space of controls. One would have\marginnote{The minimum of the future cost over the space of controls is called the value function $V(X,t)$.}
$$
\min_u C(X,u,t) = C(X,\pi(X),t) \equiv V(X,t).
$$
Clearly this formulation is too broad to allow for any useful development. One general remark to be made, though, is one first made by Richard Bellman. Bellman\cite{Bellman1952} proposed an optimality principle\marginnote{Bellman's principle of optimality}, which stated that if a given policy is an optimal solution to a control problem, than the policy resulting after a number of steps of that policy must still be optimal for the remaining control problem as well. This can be formulated as a mathematical equation, the so-called Bellman equation or dynamic programming equation, which states that the minimal future cost in state $X(t)$ at time $t$ is given by the minimum over $U(t)$ of the instantaneous cost plus the minimal future cost at the resulting future state $X_{t+dt}$. Mathematically, we have
$$
V(X(t),t) = \min_{U(t)} \left[ c(X(t),U(t),t) dt +V(X({t+dt}),t+dt)\right].
$$
Note that in general, $X({t+dt})$ will depend on $U(t)$, making the solution of the Bellman equation difficult.\par
In continuous time, one can assume differentiability of the value function $V$ in both its arguments to obtain the Hamilton-Jacobi-Bellman equation\marginnote{We will abbreviate the Hamilton-Jacobi-Bellman equation as HJB equation.}. We have
$$
V(X(t),t) = \min_{U(t)} \left[c(X(t),U(t),t) dt + V(X,t) + \frac{\partial V}{\partial t} dt + \frac{\partial V}{\partial X} dX(t) \right],
$$
which gives us then
$$
-\frac{\partial V}{\partial t} = \min_{U(t)} \left[c(X(t),U(t),t) + \frac{\partial V}{\partial X} f(X(t),U(t)) \right].
$$
This is often more convenient to solve, as it sometimes allows for explicit minimization over the control.

\subsection{Stochastic Optimal Control}

The world is a noisy place, and if we want to control real-world systems, we must be able to account for noise in the systems as well. One simple way to include noise is to generalize the system dynamics to a stochastic differential equation. We would then have
$$
dX(t) = f(X(t),U(t)) dt + H^{1/2} dW(t),
$$
where $dW(t)$ is a zero mean, unit variance Wiener process.
The HJB equation can then be calculated through It\=o's lemma, yielding
$$
-\frac{\partial V}{\partial t} = \min_{U(t)} \left[c(X(t),U(t),t) + \frac{\partial V}{\partial X} f(X(t),U(t)) + \frac{H}{2} \frac{\partial^2 V}{\partial X^2} \right].
$$
Note that we could also have a Poisson process as a noise source. If we take, for example, a Poisson counting process $N(t)$, with time-dependent rate $\lambda(t)$, and take the system dynamics to be
$$
dX(t) = f(X(t),U(t)) dt + H^{1/2} dW(t) + h(X,t) dN(t),
$$
we would have then, similarly
$$
-\frac{\partial V}{\partial t} = \min_{u} \left[c(x,u,t) + \frac{\partial V}{\partial x} f(x,u) + \frac{H}{2} \frac{\partial^2 V}{\partial x^2} + \lambda(t) \left(V(x+h(x,t),t)-V(x,t)\right)\right],
$$
now including the terms regarding the jump process.\cite{Theodorou2012,Sennewald2006} I will use this formalism to treat a simple control problem where the observations are taken from a spike train of Gaussian-tuned Poisson neurons.

\subsection{Linear-Quadratic-Gaussian Control}

The simplest stochastic control problem, is the case of a linear stochastic differential equation with linear steering and quadratic costs both in the control as in the state variables. This would mean that the evolution of the state is given by the SDE
\begin{equation}
\label{eq:ctl_diff_dyn}
dX(t) = \left(A X(t) + B U(t)\right) dt + H^{1/2} dW(t),
\end{equation}
where $W(t)$ is a Wiener process. Taking a path cost given by $c(x,u,t) = u^\top r(t) u+ x^\top q(t) x$, and a final cost given by $h(x) = x^\top Q_T x$, we can solve the problem explicitly, using the HJB equation. The HJB equation will be given by
$$
-\frac{\partial V}{\partial t} = \min_{u} \left[ u^\top r(t) u +  q(t) x^2 + \frac{\partial V}{\partial x}^\top \left(A x  + B u\right) + \frac{1}{2} \Tr\left(H\frac{\partial^2 V}{\partial x^2}\right) \right].
$$
We can minimize the right hand side explicitly and eliminate $u$ from the equation. We obtain that the optimal value of the control is given by
$$
u^*(x,t) = - r(t)^{-1} B^\top\frac{\partial V}{\partial x}\big|_{x,t}.
$$
Inserting into the HJB equation once more, we obtain
$$
-\frac{\partial V}{\partial t} = x^\top q(t) x +\frac{\partial V}{\partial x}^\top A x -\frac{\partial V}{\partial x}^\top B r(t)^{-1} B^\top \frac{\partial V}{\partial x} + \frac{1}{2}\Tr\left(H\frac{\partial^2 V}{\partial x^2}\right).
$$
We note that $V$ can only have a quadratic dependence in $X$, and we therefore assume it is of the form $V(x,t) = x^\top S(t) x + \alpha(t)^\top x + k(t)$. We then 
obtain
$$
-\dot{S} = q(t) + A^\top S(t) + S(t) A - S(t) B r(t)^{-1} B S(t)
$$
$$
-\dot{\alpha} = A^\top \alpha(t)-S(t) B r(t)^{-1} B^\top \alpha(t),
$$
$$
-\dot{k} = \Tr\left(H S(t)\right)-\alpha(t)^\top B r(t)^{-1} B^\top \alpha(t),
$$
with the terminal conditions $S(T) = Q_T$, $\alpha(T) = 0$ and $k(T)=0$. Note that the $X$ independent term $k(t)$ accounts for the future uncertainty of $X$, decreasing to $0$ over time as we approach the final time $T$. Furthermore, the differential equation for $S(t)$ is a special case of the Riccati equation.\footnote{See \ref{app:lqg} for a full account of the Riccati equation} These results can also be extended to the case of control- and state-dependent diffusion noise, affine dynamics and some other issues. For a more complete review, we refer to the work of Kappen\cite{Kappen2011}.

\section{Partially Observable Processes}

In general, one does not have access to the exact state of the system, and it is useful to consider cases where we are only given noisy observations of the state, as we have considered in the previous chapters. The most commonly considered case of partially observable control problem is a LQG problem observed through a second diffusion process. Suppose we have as above a system $X(t)$ evolving according to equation \fref{eq:ctl_diff_dyn}, but instead of observing $X(t)$ directly, we observe the process $Y(t)$, which I shall call the observation process, which evolves according to
\begin{equation}
\label{eqn:ctl_obs_dyn}
dY(t) = c X(t) dt + \eta^{1/2} dV(t).
\end{equation}
Given a control trajectory ${u(s), s\in [0,t]}$, the problem of estimating $X(t)$ given observations ${Y(s), s \in [0,t]}$, is a simple filtering problem, and is solved exactly by the Kalman-Bucy filter. We will have a Gaussian estimate of $X(t)$ with mean $\mu(t)$ and variance $\Sigma(t)$, where $\mu$ and $\Sigma$ evolve according to
\begin{subequations}
\begin{equation}
d\mu = (a \mu + b u)dt + \Sigma(t) c^t \eta^{-1} \left(dY(t) - c\mu dt\right),
\label{eq:ctl_kalman_bucy_mean}
\end{equation}
and
\begin{equation}
\label{eq:ctl_kalman_bucy_var}
\frac{d\Sigma}{dt} = a \Sigma + \Sigma a^\top + \sigma^\top \sigma - \Sigma c^\top \eta^{-1} c \Sigma.
\end{equation}
\end{subequations}
Since in this case we do not have perfect information on the process to be controlled, we have to settle for the goal of minimizing the expected cost given our observation. Therefore, we have the cost to be minimized
$$
C(u_{t_0:T};\mu_0,\Sigma_0) = E\left(\int_{t_0}^T c(X(t),U(t),t)dt +h(X(t))\right).
$$
There is no analogous to the HJB equation for the incomplete information case, but we can reformulate the problem as a control problem over the belief states, that is the state of the world as we are led to believe it is distributed given the previous observations\marginnote{The belief state is a description of an system with incomplete information which eschews describing the actual state of the system, instead describing the distribution over states. A general formulation is described in \citep{bertsekas2012}.}. In the case I am discussing, the belief state is the distribution over the state variable, given by the Gaussian distribution $\mathcal{N}(\mu(t),\Sigma(t))$. The dynamics of the belief state is then given by equations \fref{eq:ctl_kalman_bucy_mean} and \fref{eq:ctl_kalman_bucy_var}. Note that when we choose to describe the system in terms of the mean and variance of the posterior distribution, the noise process $dW(t)$ does not enter into the analysis anymore, and the observation process $dY(t)$ takes the role of the noise process. We need, however, to redefine the cost function $c(X(t),U(t),t)$ to fully specify the problem. We have that
$$
\left<c(X(t),U(t),t)\right>_{\mu(t),\Sigma(t)} = \frac{1}{2} U(t)^\top R(t) U(t) + \frac{1}{2}\left(\mu(t)^\top Q(t) \mu(t) + tr\left(Q(t)\Sigma(t)\right)\right).
$$
We can now write the HJB equation for this system. We have
$$
V(\mu(t),\Sigma(t),t) = \min_{U(t)} E\left[\frac{1}{2} U(t)^\top R(t) U(t)+\frac{1}{2}\left(\mu(t)^\top Q(t) \mu(t) + tr\left(Q(t)\Sigma(t)\right)\right)+V(\mu(t+dt),\Sigma(t+dt),t+dt)\right],
$$
where the expectation is now with respect to the observation process $Y(t)$.
Taking now the variation of $V$ with infinitesimal time increments via It\=o's lemma and minimizing over $U(t)$, we have
$$
dV = \frac{\partial V}{\partial t} dt + \frac{\partial V}{\partial \mu}^\top d\mu + Tr\left[\frac{\partial V}{\partial \Sigma}d\Sigma\right] + Tr\left[(\Sigma c^\top \eta c \Sigma)_{i,j} \frac{\partial^2 V}{\partial \mu^2}\right].
$$
Which leads to the HJB equation
\[
-\pd{V}{t} = \min_{U(t)} E\left[\frac{1}{2} U(t)^\top R(t) U(t)+\frac{1}{2}\left(\mu(t)^\top Q(t) \mu(t) + tr\left(Q(t)\Sigma(t)\right)\right) +\frac{\partial V}{\partial \mu}d\mu^\top + Tr\left[\frac{\partial V}{\partial \Sigma}d\Sigma\right] + Tr\left[(\Sigma c^\top \eta c \Sigma)_{i,j} \frac{\partial^2 V}{\partial \mu^2}\right] \right].
\]
Minimization with respect to $U(t)$ leads to $u^*(t) = -R(t)^{-1} b^\top \pd{V}{\mu}$. We will then have
\[
-\pd{V}{t} = \mu^\top Q(t)\mu + \pd{V}{\mu}^\top b R(t) b^\top \pd{V}{\mu} + tr\left(Q(t)\Sigma(t)\right) +\frac{\partial V}{\partial \mu}d\mu^\top + Tr\left[\frac{\partial V}{\partial \Sigma}d\Sigma\right] + Tr\left[(\Sigma c^\top \eta c \Sigma)_{i,j} \frac{\partial^2 V}{\partial \mu^2}\right] .
\]
\section{Partially Observable Processes with Poisson Observations}

Similarly to the case just discussed, we can consider the case of a stochastic system observed through a population of densely tuned Poisson processes with Gaussian tuning functions. The dynamics of the system would be the same as \fref{eq:ctl_diff_dyn}, but the observation processes would be given by a set of $M$ Poisson processes $N^m$ with rates given by
\begin{equation}
\label{eq:ctl_poisson_rate}
\lambda^m(X(t)) = \lambda \exp\left[-\frac{1}{2}(\theta^m-X(t))^\top A^\dagger (\theta^m-X(t))\right].
\end{equation}
As we have shown in \fref{chap:filtering}, the estimation problem is solved by the point-process analog of the Kalman-Bucy filter, first derived by Donald Snyder and used extensively since\cite{Snyder1972,Yaeli2010}. In our case, with Gaussian tuning functions, we would have the filtering equations given by
\begin{subequations}
\begin{equation}
\label{eq:ctl_poisson_mean}
d\mu(t) = (a\mu(t) + b X(t)) dt + \sum_m dN^m(t) \Sigma(t) \left(\Sigma(t) + A\right)^{-1} \left(\theta_m - \mu(t)\right) 
\end{equation}
and
\begin{equation}
\label{eq:ctl_poisson_var}
d\Sigma(t) =\left(a \Sigma(t) + \Sigma(t) a^\top + \sigma^\top\sigma\right)dt - dN(t) \Sigma(t) \left(\Sigma(t) + A\right)^{-1} \Sigma(t),
\end{equation}
\end{subequations}

where $dN(t) = \sum_m dN^m(t)$. We will define 
$$\delta \mu(t) \equiv (a\mu(t) + b X(t)) dt$$ as the continuous part of $d\mu(t)$ and 
$$\Delta^m \mu(t) \equiv \Sigma(t) \left(\Sigma(t) + A\right)^{-1} \left(\theta_m - \mu(t)\right) $$ as the jump part of $d\mu(t)$. Likewise we define
$$\delta\Sigma(t) \equiv (a\Sigma(t) + \Sigma(t) a^\top + \sigma^\top \sigma)dt$$ and $$\Delta\Sigma(t) \equiv \Sigma(t) \left(\Sigma(t) + A\right)^{-1} \Sigma(t).$$\par
These give us the evolution of the optimal Bayesian filter, provided the total rate of all the processes $\lambda (t) = \sum_m \lambda^m(X(t))$, is independent of $X(t)$. The posterior distribution over $X(t)$ given $\{N^m(s), m\in [1,\ldots,M], s\in[t_0,t]\}$, is then the normal distribution $\mathcal{N}(X(t);\mu(t),\Sigma(t))$. Assuming we are trying to minimize a cost given by the same cost rate $c(X(t),U(t),t)$ as before, we can write out the infinitesimal Bellman equation for this case as well. Since the dynamics of system and observations is Markov, we can use the posterior distribution as a sufficient statistic for our knowledge of the system. We will therefore take our belief state to be the mean and variance of our posterior distribution as before.\footnote{see \citep{bertsekas2012} for a more detailed discussion}
Similarly to the previous sections, we will consider the processes $N^m(t)$ as noise to be averaged over in the future. We will then have
$$
V(\mu(t),\Sigma(t),t)= \min_{U(t)} \left[E\left(c(X(t),U(t),t)\right)_{\mu(t),\Sigma(t)} + E\left(V(\mu_{t+dt},\Sigma_{t+dt},t+dt)\right)_{N^m(t)}\right]
$$
We can write out, according to It\=o's lemma
\begin{eqnarray*}
V(\mu_{t+dt},\Sigma_{t+dt},t+dt) =& V(\mu(t),\Sigma(t),t) + \frac{\partial V}{\partial t}dt + \frac{\partial V}{\partial \mu} \delta\mu(t) +\frac{\partial V}{\partial \Sigma} \delta \Sigma(t)\\ &+ \sum_m dN^m(t)\left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right].
\end{eqnarray*}
The expectation over the noise process $N^m(t)$ in the Bellman can then be written as
\begin{eqnarray*}
E(V_{t+dt})_{N^m(t)} =&V(t) + \frac{\partial V}{\partial t}dt + \frac{\partial V}{\partial \mu} \delta\mu(t) +\frac{\partial V}{\partial \Sigma} \delta \Sigma(t)\\ &+ \sum_m E\left(dN^m(t)\left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right]\right)_{N^m(t)} 
\end{eqnarray*}
\begin{eqnarray*}
 =&V(t) + \frac{\partial V}{\partial t}dt + \frac{\partial V}{\partial \mu} \delta\mu(t) +\frac{\partial V}{\partial \Sigma} \delta \Sigma(t)\\ &+ \sum_m E(\lambda^m(X(t)))_{\mu(t),\Sigma_n} \left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right],
\end{eqnarray*}
leading to the HJB equation
\begin{eqnarray}
-\frac{\partial V}{\partial t} &=\frac{1}{2}\mu^\top Q(t)\mu + tr\left(Q(t) \Sigma\right) +\frac{1}{2} (U(t)^*)^\top R(t) U(t)^*  + \frac{\partial V}{\partial \mu} \delta\mu +\frac{\partial V}{\partial \Sigma} \delta \Sigma \\
&+\sum_m E(\lambda^m(X(t)))_{\mu(t),\Sigma_n} \left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right]\nonumber,
\end{eqnarray}
where
\[
U(t)^* = -R(t)^{-1} b^\top \frac{\partial V}{\partial \mu}\bigg|_{\mu=\mu(t),\Sigma=\Sigma(t)}.
\]
This then leads to
\begin{eqnarray}
-\frac{\partial V}{\partial t} =&\frac{1}{2}\mu^\top Q(t)\mu + tr\left(Q(t) \Sigma\right) -\frac{1}{2} \frac{\partial V}{\partial \mu}^\top b R(t)^{-1} b^\top \frac{\partial V}{\partial \mu}  + \frac{\partial V}{\partial \mu} a \mu  \\
&+\frac{\partial V}{\partial \Sigma} \delta \Sigma+\sum_m E(\lambda^m(X(t)))_{\mu(t),\Sigma_n} \left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right]\nonumber.
\end{eqnarray}
Note that it can be shown by induction that the cost function is of form $V(\mu,\Sigma,t) =\frac{1}{2} \mu^\top S(t) \mu + f(\Sigma,t)$, given that it is of this form at the final time $T$. We can then obtain equations for $S(t)$ and $f$. We will have
\begin{equation}
\label{eq:riccatti}
-\dot{S}(t) = Q(t) - S(t) b R(t)^{-1} b^\top S(t) + S(t) a + a^\top S(t)
\end{equation}
and
\begin{equation}
-\frac{\partial f}{\partial t} = \frac{1}{2} tr\left(Q(t) \Sigma\right) + \frac{\partial f}{\partial \Sigma} \left(a\Sigma + \Sigma a^\top + \sigma^\top\sigma\right) + \bar{\lambda} \left[f(\Sigma+\Delta\Sigma,t) - f(\Sigma,t) +\frac{1}{2} tr\left(\Sigma (\Sigma+A)^{-1}\Sigma\, S(t)\right)\right].
\label{eq:f_variance}
\end{equation}
Note that \fref{eq:riccatti} is a matrix Riccatti equation, as is found in the usual LQG problem. \Fref{eq:f_variance} gives the contribution of the uncertainty of the estimate to the future costs.
\subsection{A Feynman-Kac formulation for the Uncertainty cost}

Note that the PDE for $f$ can be solved via the Feynman-Kac formula. We define the jump process
\[
d\rho(s) = (a\rho(s) + \rho(s) a^\top  + \sigma^\top \sigma) dt + \Delta \rho(s) dN(t).
\]
Defining then
\[
Y(t) = f(\rho(t),t) - \frac{1}{2} \int_t^T \left[tr\left(Q(t)\rho(s)\right)+ \bar{\lambda} tr \left(\rho(s)(\rho(s)+A)^{-1}\rho(s) S(s)\right)\right]ds,
\]
we have
\[
dY(t) = df + \frac{1}{2}\left[tr\left(Q(t)\rho(t)\right)+ \bar{\lambda} tr \left(\rho(s)(\rho(s)+A)^{-1}\rho(s) S(t)\right)\right]dt.
\]
Via the It\=o Lemma, we have
\[
df =\left(\frac{\partial f}{\partial t} + \frac{\partial f}{\partial \Sigma} (a \rho(s) + \rho(s) a^\top) + \hat{\lambda} \Delta f \right) dt+ d J(f)_s,
\]
where 
\[
\Delta f_s = f(\rho(s) + \Delta \rho(s),s) - f(\rho(s)),
\]
is the jump incurred in $f$ when there is a jump in $\rho(s)$ and $dJ(f)_s$ is the compensated process
\[
dJ(f)_s = dN(t) \Delta f_s - \hat{\lambda} \Delta f_s dt.
\]
We will then have
\[
dY(t) = \left(\frac{\partial f}{\partial t} + \frac{\partial f}{\partial \Sigma} (a \rho(s) + \rho(s) a^\top+\sigma^\top \sigma) + \hat{\lambda} \Delta f + \frac{1}{2}tr\left(Q(t)\rho(t)\right)+ \frac{\bar{\lambda}}{2} tr \left(\rho(t)(\rho(t)+A)^{-1}\rho(t) S(t)\right)\right)dt + dJ(f)_t.
\]
Note that the term in parentheses is zero, as $f$ is a solution to \fref{eq:f_variance}. Therefore, we can integrate $Y(t)$ from $t$ to $T$, obtaining
\[
Y(T) = Y(t) + \int_t^T dJ(f)_s.
\]
Taking the average with respect to the paths of process $\rho(s)$ we obtain
\[
E\left[Y_T|\rho(t)=\Sigma\right]=E\left[Y(t)|\rho(t)=\Sigma\right] + E\left[\int_t^T dJ(f)_s|\rho(t)=\Sigma\right],
\]
where the second term on the rhs vanishes, since $dJ(f)_s$ is a compensated process. We then obtain the Feynman-Kac formula for $f$
\begin{equation}
f(\Sigma,t) =E[f(\rho_T,T)|\rho(t)=\Sigma] + E\left[\int_t^T\left[tr\left(Q(t)\rho(s)\right)+ \bar{\lambda} tr \left(\rho(s)(\rho(s)+A)^{-1}\rho(s) S(s) \right)\right]ds \middle| \rho(t)=\Sigma\right].
\end{equation}
Note now, that the evolution of $\left<\Sigma(t)\right>$ is given by
\[
\pd{\left<\Sigma(t)\right>}{t} = a \left<\Sigma(t)\right> + \left<\Sigma(t)\right> a^\top + \sigma^\top\sigma- \bar{\lambda}\left< \Sigma(t)(\Sigma(t) + A)^{-1} \Sigma(t)\right>,
\]
therefore, we can use this expression to directly estimate the trace average in the equation for $f$. We have therefore
\[
\bar{\lambda} tr\left(\left<\Sigma(t)(\Sigma(t) + A)^{-1} \Sigma(t)\right>S(t)\right) = tr\left[\left(-\pd{\left<\Sigma(t)\right>}{t} + a \left<\Sigma(t)\right> + \left<\Sigma(t)\right> a^\top + \sigma^\top\sigma\right)S(t)\right]. 
\]
This prevents us from having to calculate expensive matrix inversions and allows us to write
\[
f(\Sigma,t) =E[f(\rho_T,T)|\rho(t)=\Sigma] + \int_t^T\left[ tr\left((Q(s)+S(s)a + a^\top S(s))E^t_{\Sigma}(\rho(s))\right) +tr\left(\sigma^\top \sigma S(s)\right)-tr\left( \pd{E^t_\Sigma(\rho(s))}{s} S(s)\right)\right]ds,
\]
where we have written
\[
E^t_\Sigma(X) = E[X|\rho(t)=\Sigma].
\]
Note that we can apply integration by parts to the last term to obtain
\[
\int_t^T tr\left( \pd{E^t_\Sigma(\rho(s))}{s} S(t)\right) = tr(E^t_\Sigma(\rho(s)) S(s)\big|_t^T) - \int_t^T E^t_\Sigma(\rho(s)) \dot{S}(s)ds.
\]
$\dot{S}$ in turn is given by the Riccatti equation, and results in
\[
\int_t^T tr\left( \pd{E^t_\Sigma(\rho(s))}{s} S(t)\right) =  tr(E^t_\Sigma(\rho(s)) S(s)\big|_t^T) + \int_t^T E^t_\Sigma(\rho(s)) (Q(s) + S(s) a + a^\top S(s) -S(s) b^\top R(s)^{-1} b S(s)ds.
\]
We then obtain
\[
f(\Sigma,t)  = tr\left(\Sigma(t) S(t)\right) \int_t^T tr\left(\sigma^\top\sigma S(s)\right)ds+ \int_t^T tr \left(S(s) b^\top R(s)^{-1}b S(s) E^t_\Sigma(\rho(s))\right) ds
\]

For the one-dimensional problem this becomes
\[
f(\Sigma,t) = S(t) \Sigma + \int_t^T \sigma^2 S(s) ds +\int_t^T \frac{b^2 S(s)^2 E^t_\Sigma(\rho(s))}{R(s)}ds 
\]
%\section{The Point Process Controller}

