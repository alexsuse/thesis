\newthought{Clearly the nervous system is not solely interested in estimating the state of the world.} Furthermore, if that estimate is not useful for making decisions and taking actions 
in a dynamic environment, there is little use for it. In the previous chapter I have discussed findings for spiking codes in an estimation context. In this chapter I will extend this approach 
to the framework of stochastic optimal control, and discuss how to reframe the findings in this context.\par

The field of optimal control has been of growing interest to the neuroscience community, but little attention has been given to the issue of optimal coding in a control context. Here I
will study a simple case of linear quadratic control observed through a dense population of Gauss-Poisson neurons, for which I have been able to derive a closed-form expression
for the optimal cost-to-go. This allows one to study the expected control cost in an experiment as a function of the encoder, similarly to what I have done with the MMSE in the previous 
chapters. Furthermore, in \fref{chap:optimal} I will compare these two approaches, showing that in a couple of simple examples, the control-optimal and the MMSE-optimal encoders
differ significantly.

\section{Optimal Control}
The field of control theory is concerned with the steering and controlling of systems, always with the minimization of a cost (or maximization of a reward) in mind. Speaking mathematically, given a system with state $X(t) \in \mathcal{X}$, with dynamics given by
$$
\dot{X}(t) = f(X(t),U(t)), \quad X(0) = x_0
$$
one would like to select the control variables $U(t) \in \mathcal{U}$ in such a way as to minimize an integrated cost function over time\footnote{This is an additive cost function,
which is itself only a specific kind of control problem. Generally one can also consider more complex cost functions as well, that depend on the minimum or maximum
of the state or multiplicative cost functions.}
$$
C(X_{0:T},U_{0:T}) = \int_{0}^T c(X(s),U(s),s) ds + h(X(T)).
$$
Here $c(x,u,t)$ specifies a cost rate accumulated over time and $h(x)$ describes some final goal the system should achieve at the end of the control problem.\par

In a purely deterministic setting, the solution to the control problem would be a policy $U^* : \mathcal{X} \times \mathbf{R}\to \mathcal{U}$ which for each system 
state and time gives a control to be applied to the system when it is in that state at that time. One would have\marginnote{The minimum of the future cost over the space of controls is called the value function $V(X,t)$.}
$$
\min_{U_{0:T}} C(X_{0:t},U_{0:t},0;x_0) = C\left(X_{0:t},U^*(X_{0:t}),0\right) \equiv V(x_0,0),
$$
where $V(x,t)$ is usually called the optimal cost-to-go function or the value function. $V(x,t)$ quantifies the cost one is expected to incur if he controls the system optimally through the 
remainder of the control problem, given that the system is at state $X(t)=x$ at time $t$.\par

This is a very broad formulation, but one general remark can be made, though, first put forward by Richard Bellman.\mycite{Bellman1952} Bellman 
proposed an optimality principle\marginnote{Bellman's principle of optimality}, which stated that if a given policy is an optimal solution to a control problem, then the 
policy resulting after a number of steps of that policy must still be optimal for the remaining control problem as well. This can be formulated as a mathematical equation, 
the so-called Bellman equation or dynamic programming equation, which states that the minimal future cost in state $X(t)$ at time $t$ is given by the minimum over 
$U(t)$ of the instantaneous cost plus the minimal future cost at the resulting future state $X({t+dt})$. Mathematically, we have
\begin{equation}
\label{eq:bellman_eq}
V(X(t),t) = \min_{U(t)} \left[ c(X(t),U(t),t) dt +V(X({t+dt}),t+dt)\right].
\end{equation}
Note that in general, $X({t+dt})$ will depend on $U(t)$, making the solution of the Bellman equation difficult.\par
In continuous time, assuming differentiability of the value function $V$ in both its, one obtains
\[
V(X(t),t) = \min_{U(t)} \left[c(X(t),U(t),t) dt + V(X(t),t) + \frac{\partial V}{\partial t} dt + \frac{\partial V}{\partial x} dX(t) \right],
\]
which leads to the Hamilton-Jacobi-Bellman equation\marginnote{I will abbreviate the Hamilton-Jacobi-Bellman equation as HJB equation.}
$$
-\frac{\partial V}{\partial t} = \min_{U(t)} \left[c(x,U(t),t) + \frac{\partial V}{\partial X} f(x,U(t)) \right].
$$
This is often more convenient to solve, as it sometimes allows for explicit minimisation over the control. The HJB equation must be solved backwards in time, with final condition $V(x,T) = h(x)$.

\subsection{Estimation and the Separation Principle}

In the previous two chapters, I have considered the problem of filtering a stochastic process from spike trains. More specifically, given a signal, I was looking for the optimal set of 
parameters $\varphi^*$ for a population of neurons that minimise the MMSE of the filtering problem. Here I would like to establish a similar approach to control problems. That is,
in the same sense of before, I have a noisy system observed through spike trains of a population of neurons specified by some parameters $\varphi$, but now I am concerned with
controlling this noisy system. Given a cost function, I would like to determine the parameters $\varphi^*$ that minimise the control costs, instead of the filtering error.
If one is interested in controlling a system, say a limb performing a movement, one must now deal with the uncertainties in the system and control it according to noisy estimates of its 
state. The certainty equivalence property (CEP) holds if a system one only has partial information about can be controlled ignoring the uncertainty in its state and acting as if it were 
fully observed. I will elaborate below.
\par

Consider a deterministic system
\[
\dot{X} = f(X(t),U(t)),
\]
where I have only partial knowledge about the system's state through an initial distribution $P_0(x)$ and noisy observations $Y(t)$ of the system's state. If the certainty 
equivalence property holds for this system, the optimal control for the partially-observed system, will be the optimal policy of the fully observed problem applied to the mean estimate
of the system's state. To be more precise, let me define the cost for the partially-observed system as
\[
C(P_0,Y_{0:T},U,0) = \int_0^T \boldsymbol{E}  \left[c(X(s),U(s),s) \mid Y_{0:s},P_0 \right]ds + \boldsymbol{E} \left[h(X(T))\mid Y_{0:T},P_0\right].
\]
The optimal control $U^*$ will now be a function of the observations $Y_{0:t}$ up to time $t$ and the initial distribution $P_0$. If the optimal control for the fully-observed system is 
given by $U^*_{obs}(x,t)$, the certainty equivalence property holds if the optimal control for the partially observable process is given by 
$$
U^*_{part} (Y_{0:t},P_0,t) =  U^*_{obs}\left(\boldsymbol{E}\left[X(t) \mid Y_{0:t},P_0\right],t\right).
$$
This means that the uncertainty in the system's state can be treated in two independent steps, first estimating the system's state through the posterior mean and then applying the
control as if our estimate of the state was certain. Hence the name certainty equivalence, as one applies the control as if they were certain about the system's state.
The separation property is also frequently discussed in the literature, and it is a stronger version of the certainty equivalence property, where the control $U^*_{obs}(x,t)$ being 
employed does not need
to be the optimal policy for the fully observed problem, but can be related to some other control problem with full informations.\par

One can now ask what is the encoder that minimises the expected control costs. It is tempting to conclude from the CEP that the encoder that minimises the MMSE also minimises
the control costs. This is not true, however, as I will show in \fref{chap:optimal}. I will now consider the case of stochastic optimal control, and then turn to the case of 
partially-observable stochastic optimal control. This can be treated for the case of dense Gauss-Poisson observations, and I will derive a novel relation for the optimal cost-to-go
for that case.

\section{Stochastic Optimal Control}

The world is a noisy place, and if to control real-world systems, one must be able to account for noise in the systems as well. One simple way to include noise is to generalise the system dynamics to a stochastic differential equation. Consider
$$
dX(t) = f(X(t),U(t)) dt + H^{1/2} dW(t),
$$
where $W(t)$ is a standard Wiener process. It is not possible to predict the evolution of $X(t)$ exactly anymore, so one must redefine the cost function. The natural way
to do so is to define it as the average over future states conditioned on the current state $X(t)$ and the controls to be applied $U(t)$. This will lead to
\[
C(X,U) = \boldsymbol{E} \left[\int_0^T c(X(t),U(t),t) dt \biggm\vert X(0), U_{0:T}\right].
\]
One should mention that there are other ways to deal with the stochastic nature of the problem, such as the risk-sensitive control approach,\mycite{whittle1981} where one considers
the cost function
\[
C_{\theta}(X,U) = \frac{1}{\theta} \log \boldsymbol{E} \left[\exp\left(-\theta \left(\int_0^T c(X(t),U(t),t) dt + h(X(T))\right)\right)\right].
\]
In the limit $\theta\to 0$ one recovers the former formalism. This allows one to consider risk-averse or risk-seeking control policies. I will not, however, consider this approach here.\par
The Bellman equation can then be extended to the stochastic case as
\begin{equation}
\label{eq:stochastic_bellman}
V(x,t) = \min_{U(t)}\boldsymbol{E}\left[ c\left(X(t),U(t),t\right) dt + V\left(X(t+dt),t+dt\right)\mid\,X(t)=x, U_{[t,T]}\right].
\end{equation}
Using It\=o's lemma for the variation of $V$, and averaging over the Brownian motion $dW(t)$ leads to
\[
V(x,t) = \min_{U(t)} \left[ c(x,U(t),t) dt + V\left(x,t\right)+\left(\frac{\partial V}{\partial t} dt + f(x,U(t))^\top \frac{\partial V}{\partial x} +\frac{1}{2} \Tr\left[H \frac{\partial^2 V}{\partial x^2} \right]\right)dt \right].
\]
This leads to the stochastic HJB equation
\begin{equation}
\label{eq:stochastic_HJB}
-\frac{\partial V}{\partial t} = \frac{1}{2} \Tr\left[H \frac{\partial^2 V}{\partial x^2} \right] +\min_{u} \left[ c(x,u,t)  + f(x,u)^\top \frac{\partial V}{\partial x}\right].
\end{equation}
\par
One could also consider a Poisson process as a noise source. If one takes, for example, a Poisson counting process $N(t)$, with time- and/or
 state-dependent rate $\lambda(X(t),t)$, and takes the system dynamics to be given by a drift-diffusion process with state-dependent jumps $j(X(t),t)$, occurring with
 rate $\lambda(X(t),t)$ then the SDE for the state would be,
$$
dX(t) = f(X(t),U(t)) dt + H^{1/2} dW(t) + j(X(t),t) dN(t).
$$
This would lead to the full HJB equation for a drift-jump-diffusion process controlled by some control process $U(t)$
\begin{equation}
\label{eq:martingale_HJB}
-\frac{\partial V}{\partial t} = \min_{u} \left[c(x,u,t) + f(x,u)^\top \frac{\partial V}{\partial x} + \frac{1}{2} \Tr\left[H \frac{\partial^2 V}{\partial x^2} \right] + \lambda(x,t) \left[V(x+j(x,t),t)-V(x,t)\right]\right],
\end{equation}
now including the terms regarding the jump process.\mycite{Theodorou2012,Sennewald2006} Note that the statistics of the posterior distribution of the filtering problem from the previous
chapters fit this description, namely they are a jump-drift processes with no diffusion. I will use this formalism to derive a belief state formulation of a control problem with dense
Gauss-Poisson observations.

\subsection{Linear-Quadratic-Gaussian Control}

The Linear-Quadratic-Gaussian\footnote{LQG} control problem is defined by linear dynamics in both the state and the control variable, a quadratic cost rate function
$c$ in both the state and control and a Gaussian noise source. I will treat this problem here to illustrate the optimal control formalism.
This would mean that the evolution of the state is given by the SDE
\begin{equation}
\label{eq:ctl_diff_dyn}
dX(t) = \left(A X(t) + B U(t)\right) dt + H^{1/2} dW(t),
\end{equation}
where $W(t)$ is a Wiener process. Taking a cost rate given by
$$
c(X(t),U(t),t) = U(t)^\top R(t) U(t)+ X(t)^\top Q(t) X(t),
$$
and a final cost given by $h(X(T)) = X(T)^\top Q_T X(T)$, one can solve for the value function explicitly, using the HJB equation. The HJB equation in this case will be given by
$$
-\frac{\partial V}{\partial t} = \min_{U(t)} \left[ U(t)^\top R(t) U(t) +  x^\top Q(t) x + \frac{\partial V}{\partial x}^\top \left(A x  + B U(t)\right) + \frac{1}{2} \Tr\left(H\frac{\partial^2 V}{\partial x^2}\right) \right].
$$
One can minimize the right hand side explicitly and eliminate $U$ from the equation. One obtains that the optimal control is given by
$$
U^*(x,t) = -\frac{1}{2} R(t)^{-1} B^\top\frac{\partial V}{\partial x}\big|_{x,t}.
$$
Inserting into the HJB equation once more leads to
\begin{equation}
\label{eq:lqg_hjb}
-\frac{\partial V}{\partial t} = x^\top Q(t) x +\frac{\partial V}{\partial x}^\top A x -\frac{\partial V}{\partial x}^\top B R(t)^{-1} B^\top \frac{\partial V}{\partial x} + \frac{1}{2}\Tr\left(H\frac{\partial^2 V}{\partial x^2}\right).
\end{equation}
It can be shown that $V$ can only have a quadratic dependence in $X$, since at the final time the cost is given by $h(X(N))$ which is quadratic, and the HJB equation will preserve
this property. I will assume it is of the form $V(x,t) = x^\top S(t) x + \alpha(t)^\top x + k(t)$. Inserting this into \fref{eq:lqg_hjb}
gives the ODE's for the parameters of the value function
$$
-\dot{S} = Q(t) + A^\top S(t) + S(t) A - S(t) B R(t)^{-1} B S(t),
$$
$$
-\dot{\alpha} = A^\top \alpha(t)-S(t) B R(t)^{-1} B^\top \alpha(t),
$$
$$
-\dot{k} = \Tr\left(H S(t)\right)-\alpha(t)^\top B R(t)^{-1} B^\top \alpha(t),
$$
with the terminal conditions $S(T) = Q_T$, $\alpha(T) = 0$ and $k(T)=0$. The $X$-independent term $k(t)$ accounts for the future uncertainty in $X$, decreasing to $0$ over time as we approach the final time $T$. Furthermore, the differential equation for $S(t)$ is a special case of the Riccati equation. The full optimal control for the LQG control problem will 
therefore be given by
\[
U^*(x,t) = -R(t)^{-1}B^\top S(t) x.
\]

These results can also be extended to the case of control- and state-dependent diffusion noise, affine dynamics and some other cases.\mycite{Kappen2011}

\section{Partially Observable Processes}

In general, one does not have access to the exact state of the system, and it is useful to consider cases where one is only given noisy observations of the state, as were considered in 
the previous chapters. The most commonly considered case of partially observable control problem is a LQG problem observed through a second diffusion process. Suppose one has 
as above a system $X(t)$ evolving according to \fref{eq:ctl_diff_dyn}, but instead of observing $X(t)$ directly, one observes the process $Y(t)$, which I shall call the 
observation process, given by
\begin{equation}
\label{eqn:ctl_obs_dyn}
dY(t) = C X(t) dt + D^{1/2} dV(t).
\end{equation}
Given a control trajectory $\{U(s), s\in [0,t]\}$, the problem of estimating $X(t)$ given observations ${Y(s), s \in [0,t]}$, is a simple filtering problem, and is solved exactly by the Kalman-Bucy filter.\footnote{See \fref{chap:filtering}.} It will lead to a Gaussian estimate of $X(t)$ with mean $\mu(t)$ and variance $\Sigma(t)$, where $\mu$ and $\Sigma$ evolve according to
\begin{subequations}
\label{eq:ctl_kalman_bucy}
\begin{equation}
d\mu(t) = (A \mu(t) + B U(t))dt + \Sigma(t) C^t D^{-1} \left(dY(t) - C\mu(t) dt\right),
\label{eq:ctl_kalman_bucy_mean}
\end{equation}
and
\begin{equation}
\label{eq:ctl_kalman_bucy_var}
\frac{d\Sigma}{dt} = A \Sigma + \Sigma A^\top + H - \Sigma C^\top D^{-1} C \Sigma.
\end{equation}
\end{subequations}
Since in this case we do not have perfect information on the process to be controlled, we have to settle for the goal of minimizing the expected cost given our observation. Therefore, the cost to be minimized is
$$
C(U_{0:T};\mu_0,\Sigma_0) = \boldsymbol{E}\left[\int_{t_0}^T c(X(t),U(t),t)dt +h(X(t))\right],
$$
where the average is over all future paths of $X(t)$ and all observation paths $Y(t)$.
There is no analogous to the HJB equation for the incomplete information case, but I will reformulate the problem as a control problem over the belief states, that is, the state of the world as one is led to believe it is distributed given the previous observations\marginnote{The belief state is a description of an system with incomplete information which eschews describing the actual state of the system, instead describing the distribution over states. A general formulation is described in \mycitep{bertsekas2012}.}. In the case I am discussing, the belief state is the distribution over the state variable, given by the Gaussian distribution $\mathcal{N}(\mu(t),\Sigma(t))$. The dynamics of the belief state is then given by equations \fref{eq:ctl_kalman_bucy_mean} and \fref{eq:ctl_kalman_bucy_var}. Note that when one chooses to describe the system in terms of the mean and variance of the posterior distribution, the noise process $dW(t)$ does not enter into the analysis anymore, and the observation process $dY(t)$ takes the role of the noise process. We need, however, to redefine the cost function $c(X(t),U(t),t)$ to fully specify the problem. The average cost is
$$
\boldsymbol{E}\left[c(X(t),U(t),t)\middle|\mu(t),\Sigma(t) \right] = U(t)^\top R(t) U(t) +\left(\mu(t)^\top Q(t) \mu(t) + \Tr\left(Q(t)\Sigma(t)\right)\right),
$$
from which one can define a belief-state cost rate, which makes no mention of the underlying unobservable process
\[
c(\mu,\Sigma,U,t) =  U(t)^\top R(t) U(t) +\mu(t)^\top Q(t) \mu(t) + \Tr\left(Q(t)\Sigma(t)\right).
\]
One can now write the HJB equation for the system described by \fref{eq:ctl_kalman_bucy}, leading to
$$
V(\mu(t),\Sigma(t),t) = \min_{U(t)} \boldsymbol{E}\left[ U(t)^\top R(t) U(t)+\left(\mu(t)^\top Q(t) \mu(t) + \Tr\left(Q(t)\Sigma(t)\right)\right)+V(\mu(t+dt),\Sigma(t+dt),t+dt)\right],
$$
where the expectation is now with respect to the observation process $Y(t)$.
Taking the variation of $V$ with infinitesimal time increments via It\=o's lemma one has
$$
dV = \frac{\partial V}{\partial t} dt + \frac{\partial V}{\partial \mu}^\top d\mu + \Tr\left[\frac{\partial V}{\partial \Sigma}d\Sigma\right] +\frac{1}{2} \Tr\left[(\Sigma C^\top D^{-1} C 
\Sigma)_{i,j} \frac{\partial^2 V}{\partial \mu^2}\right],
$$
which leads to the HJB equation
\begin{eqnarray*}
-\pd{V}{t} =& \min_{U(t)} \boldsymbol{E}\left[ U(t)^\top R(t) U(t)+\mu(t)^\top Q(t) \mu(t) + \Tr\left(Q(t)\Sigma(t)\right) +\frac{\partial V}{\partial \mu}d\mu^\top\right] \\&+ \Tr\left[\frac{\partial V}{\partial \Sigma}d\Sigma\right] +\frac{1}{2} \Tr\left[(\Sigma C^\top D^{-1} C \Sigma)_{i,j} \frac{\partial^2 V}{\partial \mu^2}\right] .
\end{eqnarray*}
Minimization with respect to $U(t)$ leads to $U^*(t) = -R(t)^{-1} B^\top \pd{V}{\mu}$, which results in
\begin{eqnarray}
-\pd{V}{t} =& \mu^\top Q(t)\mu + \pd{V}{\mu}^\top B R(t) B^\top \pd{V}{\mu} + \Tr\left(Q(t)\Sigma(t)\right) +\frac{\partial V}{\partial \mu}d\mu^\top\nonumber\\&+ \Tr\left[\frac{\partial V}{\partial \Sigma}d\Sigma\right] +\frac{1}{2} \Tr\left[(\Sigma C^\top D^{-1} C \Sigma)_{i,j} \frac{\partial^2 V}{\partial \mu^2}\right] .
\label{eq:lqg_hjb_partial}
\end{eqnarray}
This would now have to be solved backwards from $V(\mu,\Sigma,T) = \mu^\top Q_T \mu + \Tr[\Sigma Q_T]$. \Fref{eq:lqg_hjb_partial} provides a clean formulation of the control problem in 
terms of the belief state, where the underlying process has been integrated over completely. This is a very useful approach and I will leverage it for the case of Point processes
observations below.
If I write the value function as $V(\mu,\Sigma,t) = \mu^\top S(t) \mu + f(\Sigma,t)$, I will obtain the same Riccati equation for $S(t)$ as in the fully observed case. Using this form for
the value function, one immediately recovers the optimal control $U^*(t) = -R^{-1}(t) B^\top S(t) \mu$, which shows the certainty equivalence property for this system.
\section{Partially Observable Processes with Poisson Observations}

Similarly to the case just discussed, we can consider the case of a stochastic system observed through a population of densely tuned Poisson processes with Gaussian tuning functions. The dynamics of the system would be the same as \fref{eq:ctl_diff_dyn}, but the observation processes would be given by a set of $M$ Poisson processes $N^m$ with rates given by
\begin{equation}
\label{eq:ctl_poisson_rate}
\lambda^m(X(t)) = \lambda \exp\left[-\frac{1}{2}(\theta_m-X(t))^\top \covar^\dagger (\theta_m-X(t))\right],
\end{equation}
where the tuning centres $\theta_m$ are positioned in such a way that the overall firing rate of the population $\hat{\lambda} = \sum_m \lambda^m(X(t))$ is independent of the system's
state $X(t)$.
As we have shown in \fref{chap:filtering}, the estimation problem is solved by the point-process analog of the Kalman-Bucy filter, first derived by Donald Snyder.\mycite{Snyder1972,Bobrowski2009} In the present case, with Gaussian tuning functions, the filtering equations are
\begin{subequations}
\begin{eqnarray}
d\mu(t) =& (A\mu(t) + B X(t)) dt \nonumber\\
+& \sum_i  \left[\Sigma(t^-)\left(I+\covar^\dagger \Sigma(t^-)\right)^{-1} \covar^\dagger\left(\theta_i - \mu(t^-)\right)\right]dN^i(t)\nonumber\\ 
\label{eq:ctl_poisson_mean}
\end{eqnarray}
and
\begin{eqnarray}
d\Sigma(t) =&\left(A\Sigma(t) + \Sigma(t) A^\top + H\right)dt\nonumber \\
-&  \left[\Sigma(t^-) \covar^\dagger \Sigma(t^-) \left(I+\covar^\dagger \Sigma(t^-)\right)^{-1}\right] dN(t),
\label{eq:ctl_poisson_var}
\end{eqnarray}
\end{subequations}

where $dN(t) = \sum_m dN^m(t)$. I will define 
$$
\delta \mu(t) \equiv (A\mu(t) + B X(t)) dt
$$
as the continuous part of $d\mu(t)$ and 
$$
\Delta^i \mu(t) \equiv dN^i(t) \left[\Sigma(t^-)\left(I+\covar^\dagger \Sigma(t^-)\right)^{-1} \covar^\dagger\left(\theta_i - \mu(t^-)\right)\right]
$$
as the jump part of $d\mu(t)$. Likewise, for the variance, define
$$
\delta\Sigma(t) \equiv (A\Sigma(t) + \Sigma(t) A^\top + H)dt
$$ 
and
$$
\Delta\Sigma(t) \equiv dN(t) \left[\Sigma(t^-) \covar^\dagger \Sigma(t^-) \left(I+\covar^\dagger \Sigma(t^-)\right)^{-1}\right].
$$\par
These give the evolution of the optimal Bayesian filter, with the posterior distribution over $X(t)$ conditioned on the observations $\{N^m(s), m\in [1,\ldots,M], s\in[t_0,t]\}$,
given by the normal distribution $\mathcal{N}(X(t);\mu(t),\Sigma(t))$. Assuming one is trying to minimize a cost given by the same cost rate $c(X(t),U(t),t)$ as before, one canwrite out 
the infinitesimal Bellman equation for this case as well. Since the dynamics of the system and the observations is Markov, I can use the posterior distribution as a sufficient statistic for 
the knowledge of the system's state. I will therefore take the belief state to be the mean and variance of the posterior distribution as before.\footnote{See \mycitep{bertsekas2012} for a 
more  detailed discussion.}
Similarly to the previous sections, I will consider the processes $N^m(s), s\le t$ as noise to be averaged over in the future. This leads to
$$
V(\mu(t),\Sigma(t),t)= \min_{U(t)} \left\{\boldsymbol{E}_{X(t)}\left[c(X(t),U(t),t)\right] + \boldsymbol{E}_{\{N^m(t)\}}\left[V(\mu({t+dt}),\Sigma(t+dt),t+dt)\right]\right\}
$$
According to It\=o's lemma, one obtains
\begin{eqnarray*}
V(\mu({t+dt}),\Sigma(t+dt),t+dt) =& V(\mu(t),\Sigma(t),t) + \frac{\partial V}{\partial t}dt + \frac{\partial V}{\partial \mu} \delta\mu(t) +\Tr\left[\frac{\partial V}{\partial \Sigma} \delta \Sigma(t)\right]\\ &+ \sum_m dN^m(t)\left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right].
\end{eqnarray*}
The expectation over the noise process $N^m(t)$ in the Bellman equation can then be written as
\begin{eqnarray*}
\boldsymbol{E}(V_{t+dt})_{N^m(t)} =&V(t) + \frac{\partial V}{\partial t}dt + \frac{\partial V}{\partial \mu} \delta\mu(t) +\Tr\left[\frac{\partial V}{\partial \Sigma} \delta \Sigma(t)\right]\\ +& \sum_m \boldsymbol{E}_{N^m(t)}\left[dN^m(t)\left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right]\right] \\
 =&V(t) + \frac{\partial V}{\partial t}dt + \frac{\partial V}{\partial \mu}^\top \delta\mu(t) +\Tr\left[\frac{\partial V}{\partial \Sigma} \delta \Sigma\right]\\ +& \sum_m \boldsymbol{E}_{X(t)}[\lambda^m(X(t))] \left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right],
\end{eqnarray*}
leading to the HJB equation
\begin{eqnarray}
-\frac{\partial V}{\partial t} &=\mu^\top Q(t)\mu + \Tr\left(Q(t) \Sigma\right) +(U(t))^\top R(t) U(t)  + \frac{\partial V}{\partial \mu}^\top \delta\mu +\Tr\left[\frac{\partial V}{\partial \Sigma} \delta \Sigma\right] \\
&+\sum_m \boldsymbol{E}_{X(t)}\left[\lambda^m(X(t))\right]\left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right]\nonumber.
\end{eqnarray}
Minimisation with respect to the control, gives us the optimal control policy
\[
U^*(t) = -R(t)^{-1} B^\top \frac{\partial V}{\partial \mu}\bigg|_{\mu=\mu(t),\Sigma=\Sigma(t)}.
\]
This yields
\begin{eqnarray}
-\frac{\partial V}{\partial t} =&\mu^\top Q(t)\mu + \Tr\left(Q(t) \Sigma\right) - \frac{\partial V}{\partial \mu}^\top B R(t)^{-1} B^\top \frac{\partial V}{\partial \mu}  + \frac{\partial V}{\partial \mu} A \mu  \\
&+\Tr\left[\frac{\partial V}{\partial \Sigma} \delta \Sigma\right]+\sum_m \boldsymbol{E}_{X(t)}\left[\lambda^m(X(t))\right] \left[V\left(\mu(t) +\Delta^m\mu(t) , \Sigma(t)+\Delta\Sigma(t),t\right)-V(\mu(t),\Sigma(t),t)\right]\nonumber.
\end{eqnarray}
It can be shown that the optimal cost-to-go function is of form $V(\mu,\Sigma,t) = \mu^\top S(t) \mu + f(\Sigma,t)$, since it is of this form at the final time $T$ because 
of the final cost $h(x) = x^\top Q_T x$. I can now write down the equations for $S(t)$ and $f(\Sigma,t)$. The equation for $S(t)$ is the same as for 
the LQG case
\begin{equation}
\label{eq:riccatti}
-\dot{S}(t) = Q(t) - S(t) b R(t)^{-1} b^\top S(t) + S(t) a + a^\top S(t).
\end{equation}
The equation for $f(\Sigma,t)$ can be shown to be\footnote{See \fref{app:f_sigma}.}
\begin{equation}
-\frac{\partial f}{\partial t} = \Tr\left(Q(t) \Sigma\right) + \frac{\partial f}{\partial \Sigma} \left(A\Sigma + \Sigma A^\top + H\right) + \hat{\lambda} \left[f(\Sigma+\Delta\Sigma,t) - f(\Sigma,t) + \Tr\left(\Sigma S(t) \Sigma \left(\Sigma+\covar\right)^{-1}\right)\right].
\label{eq:f_variance}
\end{equation}
\Fref{eq:f_variance} gives the contribution of the uncertainty of the estimate to the future costs. This allows one to quantify the effect of our encoder on the control costs. In
\fref{chap:optimal} I will use $f$ to determine the optimal encoding strategies for a simple control problem. \Fref{eq:f_variance} can be shown to be solved by
\begin{equation}
\label{eq:poiss_cost}
f(\Sigma,t)  = \Tr\left(\Sigma(t) S(t)\right) + \int_t^T \Tr\left(H S(u)\right)du+ \int_t^T \Tr \left(S(u) B^\top R(u)^{-1}B S(u) \boldsymbol{E}\left[\Sigma(u)\mid \Sigma(t) = \Sigma \right]\right) du.
\end{equation}
where the expectation is over all paths of \fref{eq:ctl_poisson_var} with initial condition $\Sigma(t) = \Sigma$. I provide a derivation of this result based on the Feynman-Kac formula in
\fref{app:feynman_kac}.
This equation allows one to separate the different ways in which the uncertainty affects the expected future cost. The first term accounts for the uncertainty in the present estimate
of the system's state. The second term is due to the stochastic nature of the stimulus $X(t)$, and describes the accumulation of uncertainty due to the Brownian noise in that process. 
The third term accounts for the effect of the uncertainty on the applied control. If one is uncertain of the system's state, the control applied will not be exactly the optimal for the system's 
state, and additional costs will be incurred because of that. The third term is also the only one that depends on the parameters of the encoder, more specifically it depends on the
future dynamics of the posterior covariance $\Sigma(t)$, which in turn depends on the firing rates and the tuning widths. A similar relation can be derived for the LQG case as 
well,\footnote{See \citep[p. 290]{astrom2006} for the full derivation.} but
the full result for the partially observable control with Point process observations is novel.
\par

From the derivation above, it follows that the optimal control is again given by
\[
U^*(t) = -R(t)^{-1} B^\top S(t) \mu(t),
\]
showing that the certainty equivalence property holds in this case as well. I will discuss these issues further in \fref{sec:optimal_code_control}.\par
The finding that the certainty equivalence
property holds in this simple set up, along with the exact expression for the optimal cost-to-go has not been shown in the literature to the best of my knowledge, and I believe it to
provide a good starting point for the study of optimal codes in a control-theoretical setting.


%
%Below I will derive a Feynman-Kac formulation to solve for the uncertainty-related cost $f(\Sigma,t)$. This is an interesting relation, as it allows one to write the
%uncertainty-related costs of a given encoder as an average over all future observation paths. Furthermore it illustrates the application of the Feynman-Kac formula,
%a very important tool in the field of stochastics. In this simple case, however, one can resort to a simpler derivation, based on a lemma from \mycitep{astrom2006}. One
%can easily show that if $S(t)$ is the solution of the Ricatti \fref{eq:riccatti}, and the system $X(t)$ evolves according to \fref{eq:ctl_diff_dyn}, we have the simple
%relationship
%\begin{flalign}
%&X(T)^\top Q_T X(T) + \int_t^T \left[X(s)^\top Q(s) X(s)+U(s)^\top R(s) U(s) \right] ds\nonumber\\
%=&X(t)^\top S(t) X(t) +\int_t^T (U(s) + R(t)^{-1} B^\top S(s) X(s))^\top R(t)(U(s) + R^{-1} B^\top S(s) X(s))ds \nonumber\\
%+& \int_t^T \Tr(D S(s)) ds + \int_t^T dW(s)^\top D^{\top/2} S(s) X(s) + \int_t^T X(s)^\top S(s) D^{1/2} dW(s).
%\label{eq:lemma_astrom}
%\end{flalign}
%The left hand-side of \fref{eq:lemma_astrom} gives us the cost function of the control problem. We can then simply take the expectation over the states and
%future observations conditioned on the initial condition to obtain the expected future cost. The terms containing $dW(s)$ will vanish, as they are Brownian
%stochastic integrals, yielding
%\begin{flalign}
%&\boldsymbol{E}\left[X(T)^\top Q_T X(T) + \int_t^T \left[X(s)^\top Q(s) X(s)+U(s)^\top R(s) U(s) \right] ds\right]\nonumber\\
%=&\mu(t)^\top S(t) \mu(t) +\Tr\left[\Sigma(t)S(t)\right] +\int_t^T (U(s) + R(t)^{-1} B^\top S(s) \mu(s))^\top R(t)(U(s) + R^{-1} B^\top S(s) \mu(s))ds \nonumber\\
%+& \int_t^T \Tr(D S(s)) ds +\boldsymbol{E}\left\{ \int_t^T \Tr\left[S(s) B R(s)^{-1} B S(s) \Sigma(s)\right]ds\right\}.
%\end{flalign}
%The only term that depends on the encoder is the future average of $\Sigma(s)$ in the last term. We can write the uncertainty-dependent cost as
%\[
%f(\Sigma,t) = \int_t^T  \Tr\left[S(s) B R(s)^{-1} B S(s) \boldsymbol{E}[\Sigma(s)|\Sigma(t)]\right]ds,
%\]
%where the expectation is over all possible observation paths between $t$ and $s$ conditioned on the initial belief state $\mathcal{N}(\mu(t),\Sigma(t))$. This result also
%highlights the separability of the problem, as it is immediately clear from the expression above they the optimal control is given by the value of $U(t)$ which minimises
%the expression above. In this case, we simply take
%\[
%U^*(t) = -R(t)^{-1} B^\top S(s) \mu(t).
%\]
%%\section{The Point Process Controller}
%
